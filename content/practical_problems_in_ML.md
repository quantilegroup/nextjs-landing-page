---
title: Practical Data Issues
image: https://miro.medium.com/max/2684/1*SmEVPeZABqoMRnwXJKy0Pg.jpeg
date: 2019-10-29
description: "Underlying data issues to conquer beforebuilding your first model"
image_alt_text: Jacek Dylag via Unsplash
---

Picture this: we’ve just come up with an ingenious use case for machine learning at our company or client. When we bring the idea to our team, they’re immediately intrigued; not only would our idea be straightforward to build, but it comes with a clear business case and the opportunity to solve a reoccurring pain point in our users’ lives. Our executive stakeholders are just as enthused and immediately grant us the funding we need to develop a small-scale pilot. Before we know it, we’re getting pings and emails from managers and data scientists on other teams who have heard about what we’re whipping up and are wondering how they can get involved. “It’s clear we’ve got a winner on our hands,” you hear someone say, “if only we can find the data”.

This is where the rose-colored glasses come off. Accessing clean, reliable data is always the most difficult stage of the machine learning workflow. Many people are surprised to learn that procuring, exploring, and transforming data can take up more than 70% of our pilot’s total development time, and some are quick to look for shortcuts around this process. As we’ll soon see, we can’t afford to skimp on this critical step: the issues we’re looking to identify at this stage are extremely nuanced, difficult to identify, and showstopping if left unresolved.

## Access Issues

Before we can do anything else, we have to find where the data lives and who can provide us access. This step is usually harder than it sounds. Large enterprises will have well-structured data sitting in a dozen different silos (each with their own owner), while startups and smaller companies lack the time and funding to invest in convenient database infrastructure. If we’re lucky, we’ll be able to work with daily users to map the systems we need to pull from and gain access within a few days. If not, we’ll spend the next week tracking-down data owners and filling-out access requests before finally gaining access to the appropriate back-ends.

## Biases, Preprocessing, and Encoding Issues

The real work begins after we connect the various sources we need to our development environment. This next step, often called data wrangling or data cleaning, is the iterative process of exploring, joining, and transforming our data to ensure that it’s both ready for modeling and representative of reality. This process isn’t nearly as glamorous as incorporating predictive models into our ML pipeline, but it’s easily the most important stage in the machine learning workflow. As [Amazon’s recent foray](https://www.inc.com/guadalupe-gonzalez/amazon-artificial-intelligence-ai-hiring-tool-hr.html) into automated recruiting demonstrates, any model trained on biased data will produce biased results. Teasing-out these biases prior to modeling is vital to the success of our pilot.

The tells we’re looking for in this stage are easy to spot: erroneous or missing values, duplicate or mislabeled entries, unexpected distributions, and extreme outliers are all visible in basic summary statistics. The presence of one or more of these little issues, however, can signal a deeper problem that may not have an easy solution:

* **Sampling biases**: Our training data excludes a particular subgroup or generally isn’t representative of the overall population, so our models won’t generalize well to the real world. The Amazon pilot mentioned above is an excellent example of this issue. If you train a hiring model using data that is disproportionately male, it’s going to incorrectly assume that men are better hires than women. We can correct some biases using resampling if we have enough data, but this issue can be a non-starter if the biases are too severe.
* **Off-by-one errors**: One of our upstream loops, joins, or maps didn’t behave how we expected and, as a result, one of our variables no longer means what we think it means. This issue can be incredibly difficult to diagnose and often remains undiscovered until someone attempts to interpret the errant model. If the error occurred in the data collection or ETL stages, then we may have to involve the data owner to find a fix.
* **Nonstationarity**: The distribution of a particular feature changes over time, so we’ll have to consider the time range we use to train our models. One common example is weekly sales, which tends to change over time as a product moves through different stages of its life cycle. If we can logically explain the nonstationarity, we may be able to model our data using a few simple statistical tricks. If we don’t understand the reason behind the change, however, we may be missing important environmental variables in our dataset.
* **Heteroscedasticity**: The variance of one of our features changes as our target variable changes, indicating measurement error or sub-population differences within our training data. Similar to nonstationarity, the presence of heteroscedasticity may suggest that we need to find or generate new features prior to modeling.
* **Skewed variables**: Our variables exhibit a strong left or right-leaning skew which could cause our models to overfit on outliers if we don’t transform our data prior to training. Common transformations to combat this issue include standardization, normalization, or log-transformation; we’ll have to experiment with several different scaling methods to find which one works best for our data.
* **Improper encodings**: The way we’ve encoded or indexed our variables is causing our models to make assumptions that aren’t indicative of reality. Using a linear scale alone to represent each month, for example, won’t accurately capture the cyclical nature of time (December and January are next to each other, not 12–1 = 11 months apart). Using indexed encodings may not be a big deal if we’re sticking to tree-based algorithms (LightGBM, in particular, is amazing at handling categoricals), but other techniques will suffer if we don’t generate new features to capture these relationships.

These problems have no right answer, no magic bullet; there are a half-dozen ways to deal with each, and every method requires a different trade-off. To pick the best approach, we’ll need to continually generate hypotheses about where our data comes from, test those hypotheses using visualizations and statistics, and validate our assumptions with the data’s owner(s). This process is both tedious and time-consuming, but it’s meant to be more of an iterative process throughout our build: we’ll find some issues up front, but many will remain uncovered until after we’ve started interpreting our models. What matters most is that we stay in exploration mode for as long as possible and build-in enough time to test and resolve these issues without missing deadlines.

One thing’s for certain: when our users and executives are expecting to make decisions based on our work, we can’t glaze-over this analysis and expect to deliver a successful pilot. The importance of exploring and understanding our data is also why I’m bearish on all of the [AutoML hype](https://www.forbes.com/sites/janakirammsv/2018/04/15/why-automl-is-set-to-become-the-future-of-artificial-intelligence/#11410a0d780a); today’s offerings aren’t anywhere near intelligent enough to replicate the functional experience and critical thinking required to catch these issues in the wild. Amazon’s HR hiccup was relatively minor because they caught it early, but what happens when less informed companies move their biased AutoML models into production? We’ll have countless examples to explore in the coming years.